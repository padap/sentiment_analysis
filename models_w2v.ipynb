{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec в качестве признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.tokenize as word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\") # Choose a language\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скользящий контроль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_fold import get_fold #разбивка на выборку с нормальным распределением\n",
    "def getTrainTest(seed = 42):  #создать тестовую и тренировучную выборку\n",
    "    X_train = pd.concat([get_fold(K_fold=i,seed = seed) for i in range(7)])\n",
    "    X_test  = pd.concat([get_fold(K_fold=i,seed = seed) for i in range(7,10)])\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec, обученный на википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_fpath = \"w2v/all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec, обученный на постах вконтакте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(a, encoding=\"utf8\"):\n",
    "    if isinstance(a, bytes):\n",
    "        return a.decode(encoding)\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "    return [decode(i) for i in tokens]\n",
    "\n",
    "\n",
    "def tokenize_w2v(file_text):   #Токенизация предложений\n",
    "    tokens = nltk.word_tokenize(file_text, language='english')\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    tokens = [i.replace(u\"«\", u\"\").replace(u\"»\", u\"\") for i in tokens]\n",
    "    tokens_new = []\n",
    "    for i in tokens:\n",
    "#         if i in w2v.wv:\n",
    "        smiles = re.findall(emoji.get_emoji_regexp(), i)\n",
    "        while '' in smiles:\n",
    "            smiles.remove('')\n",
    "        if len(smiles) == 0:\n",
    "            tokens_new.append(i)\n",
    "        else:\n",
    "            temp = i\n",
    "            for s in smiles:\n",
    "                temp = temp.replace(s,'')\n",
    "            if temp != '':\n",
    "                tokens_new.extend([temp]+smiles)\n",
    "            else:\n",
    "                tokens_new.extend(smiles)\n",
    "    return tokens_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение на постах вконтакте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n"
     ]
    }
   ],
   "source": [
    "# # Обучение на постах вконтакте\n",
    "import json\n",
    "posts = json.load(open('data/posts_list.txt','r'))\n",
    "print('loading posts')\n",
    "sentences = []\n",
    "for i in tqdm(posts):\n",
    "    sentences.append(tokenize_w2v(decode(i).lower()))\n",
    "print('making sentences')\n",
    "import gensim\n",
    "w2v = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)\n",
    "print('model')\n",
    "w2v.wv.save_word2vec_format('w2v/model_w2v_vk.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка предобученной модели вконтакте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('w2v/model_w2v_vk.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логгирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = ['accuracy_score', 'f1_score', 'precision_score', 'recall_score']\n",
    "types =   ['0/1/-1', '0/1,-1','1/0,-1','-1/0,1']\n",
    "\n",
    "def make_log(y_predict, y_test, seed = 42):\n",
    "    text =  \"\\n\\nseed\\t\"+str(seed)\n",
    "    text += \"\\n'3 class classification\\t0/1/-1\"\n",
    "    text += \"\\naccuracy_score\\t\"+str(accuracy_score(y_test, y_predict))\n",
    "    text += \"\\nf1_score\\t\"+str(f1_score(y_test, y_predict, average = 'micro'))\n",
    "    text += \"\\nprecision_score\\t\"+str(precision_score(y_test, y_predict, average = 'micro'))\n",
    "    text += \"\\nrecall_score\\t\"+str(recall_score(y_test, y_predict, average = 'micro'))\n",
    "    \n",
    "    text += \"\\n2 class classification\\t0/1,-1\"\n",
    "    lbl = 0\n",
    "    text += \"\\naccuracy_score\\t\"+str(accuracy_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nf1_score\\t\"+str(f1_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nprecision_score\\t\"+str(precision_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nrecall_score\\t\"+str(recall_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "\n",
    "    \n",
    "    text += \"\\n2 class classification\\t1/0,-1\"\n",
    "    lbl = 1\n",
    "    text += \"\\naccuracy_score\\t\"+str(accuracy_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nf1_score\\t\"+str(f1_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nprecision_score\\t\"+str(precision_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nrecall_score\\t\"+str(recall_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "\n",
    "    text += \"\\n2 class classification\\t-1/0,1\"\n",
    "    lbl = -1\n",
    "    text += \"\\naccuracy_score\\t\"+str(accuracy_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nf1_score\\t\"+str(f1_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nprecision_score\\t\"+str(precision_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "    text += \"\\nrecall_score\\t\"+str(recall_score([i==lbl for i in  y_test], [i==lbl for i in  y_predict]))\n",
    "   \n",
    "    text += \"\\nmatrix_cross_valid\\n\"\n",
    "    M = np.zeros((3,3), dtype = int)\n",
    "    for i in zip(y_test, y_predict):\n",
    "        t1, t2 = i\n",
    "        if i[0] == -1:\n",
    "            t1 = 2\n",
    "        if i[1] == -1:\n",
    "            t2 = 2\n",
    "        M[t1, t2] += 1\n",
    "    text += str(M[0,0])+','+str(M[0,1])+','+str(M[0,2])+'\\n'\n",
    "    text += str(M[1,0])+','+str(M[1,1])+','+str(M[1,2])+'\\n'\n",
    "    text += str(M[2,0])+','+str(M[2,1])+','+str(M[2,2])+'\\n'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание выборки для обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def decode(a, encoding=\"utf8\"):\n",
    "    if isinstance(a, bytes):\n",
    "        return a.decode(encoding)\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "    return [decode(i) for i in tokens]\n",
    "\n",
    "def tokenize_me(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text, language='english')\n",
    "    tokens = [i.replace(u\"«\", u\"\").replace(u\"»\", u\"\") for i in tokens]\n",
    "    tokens_new = []\n",
    "    for i in tokens:\n",
    "        smiles = re.findall(emoji.get_emoji_regexp(), i)\n",
    "        while '' in smiles:\n",
    "            smiles.remove('')\n",
    "        if len(smiles) == 0:\n",
    "            tokens_new.append(i)\n",
    "        else:\n",
    "            temp = i\n",
    "            for s in smiles:\n",
    "                temp = temp.replace(s,'')\n",
    "            if temp != '':\n",
    "                tokens_new.extend([temp]+smiles)\n",
    "            else:\n",
    "                tokens_new.extend(smiles)\n",
    "    tokens = []\n",
    "    for w in tokens_new:\n",
    "        if w in w2v.wv:\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "def get_words_matrix(df, show_progress = True):\n",
    "    all_words = []\n",
    "    if show_progress:\n",
    "        for i in tqdm(df['data'].get_values(), leave = False):\n",
    "            words = []\n",
    "            words = tokenize_me(decode(i).lower())\n",
    "            all_words.extend(words)   \n",
    "    else:\n",
    "        for i in df['data'].get_values():\n",
    "            words = []\n",
    "            words = tokenize_me(decode(i).lower())\n",
    "            all_words.extend(words)   \n",
    "    all_words = Counter(all_words)\n",
    "    all_words = dict(filter(lambda x:x[1]>1 and x[1]<250, all_words.items()))\n",
    "    print(\"Len: \"+str(len(all_words)))\n",
    "    return list(all_words.keys())\n",
    "\n",
    "def get_X_matrix(df, all_words, show_progress = False, dtype = 1):\n",
    "    X_train = np.zeros((len(df), w2v.wv.word_vec('прив').shape[0]))\n",
    "    if show_progress:\n",
    "        for c,i in tqdm(enumerate(df['data'].get_values()), leave = False):\n",
    "            words = []\n",
    "            words = tokenize_me(decode(i).lower())\n",
    "            for w in words:\n",
    "                if w in all_words:\n",
    "                    ind = all_words.index(w)\n",
    "                    X_train[c] += w2v.wv.word_vec(w)\n",
    "    else:\n",
    "        for c,i in enumerate(df['data'].get_values()):\n",
    "            words = []\n",
    "            words = tokenize_me(decode(i).lower())\n",
    "            for w in words:\n",
    "                if w in all_words:\n",
    "                    ind = all_words.index(w)\n",
    "                    X_train[c] += w2v.wv.word_vec(w)\n",
    "    return X_train      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть в качестве алгоритма обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def get_label(lst):\n",
    "    ind = lst.argmax()\n",
    "    if ind == 0:\n",
    "        return 0\n",
    "    elif ind == 1:\n",
    "        return 1\n",
    "    elif ind == 2:\n",
    "        return -1\n",
    "    \n",
    "def NN(X_train, y_train, X_test, y_test, verbose = 0, epochs = 50):\n",
    "\tN_out = 3\n",
    "\tN_in  = X_train.shape[1]\n",
    "\tX_all = np.concatenate((X_train,X_test))\n",
    "\ty_all = keras.utils.to_categorical(np.concatenate((y_train,y_test)), num_classes=N_out)\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(512, activation='relu', input_dim=N_in))\n",
    "# \tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "# \tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(N_out, activation='softmax'))\n",
    "\tsgd = SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t              optimizer=sgd,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "# \t#### Draw model graph\n",
    "# \tfrom keras.utils.vis_utils import plot_model  \n",
    "# \tplot_model(model, to_file='model.png', show_shapes=True)  \n",
    "# \t#####################\n",
    "\n",
    "\n",
    "\thistory = model.fit(X_all,\n",
    "\t \t \t\t \t \ty_all, \n",
    "\t \t \t \t \t \tvalidation_split = 0.3,\n",
    "\t \t \t \t \t \tepochs=epochs,\n",
    "\t \t \t \t \t \tbatch_size=64,\n",
    "\t \t \t \t \t \tverbose=verbose,\n",
    "\t \t \t \t \t \tshuffle=False)\n",
    "\tprint('===')  \n",
    "\ty_predict = [get_label(k) for k in model.predict(X_test, batch_size=32, verbose=0)]\n",
    "\treturn y_predict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерфейс для логистической регрессии, SVM и нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogisticRegressionScore(X_train, y_train, X_test, y_test):\n",
    "    regr = LogisticRegression(C = 0.4)\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_predict = regr.predict(X_test)\n",
    "    temp = make_log(y_predict, y_test)\n",
    "    return temp\n",
    "def SVMScore(X_train, y_train, X_test, y_test):\n",
    "    SVM_model = LinearSVC(C = 0.05)\n",
    "    SVM_model.fit(X_train, y_train)\n",
    "    y_predict = SVM_model.predict(X_test)\n",
    "    temp = make_log(y_predict, y_test)\n",
    "    return temp\n",
    "\n",
    "def NNScore(X_train, y_train, X_test, y_test, verbose = 1, epochs = 10):\n",
    "    y_predict = NN(X_train, y_train, X_test, y_test, verbose = verbose, epochs=epochs)\n",
    "    temp = make_log(y_predict, y_test)\n",
    "    return temp\n",
    "\n",
    "def LR_SVM_NN_logs(X_train, y_train, X_test, y_test):\n",
    "    return (LogisticRegressionScore(X_train, y_train, X_test, y_test), \n",
    "            SVMScore(X_train, y_train, X_test, y_test), \n",
    "            NNScore(X_train, y_train, X_test, y_test)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание признаков, обучение алгоритмов и логирование результатов в файлы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:\t0\n",
      "Len: 9104\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t1\n",
      "Len: 9275\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t2\n",
      "Len: 9116\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t3\n",
      "Len: 9466\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t4\n",
      "Len: 9610\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t5\n",
      "Len: 9028\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t6\n",
      "Len: 9773\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t7\n",
      "Len: 9646\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t8\n",
      "Len: 9681\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t9\n",
      "Len: 9691\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t10\n",
      "Len: 9510\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t11\n",
      "Len: 9194\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t12\n",
      "Len: 9200\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t13\n",
      "Len: 9818\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t14\n",
      "Len: 9334\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t15\n",
      "Len: 9190\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t16\n",
      "Len: 9641\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t17\n",
      "Len: 9441\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t18\n",
      "Len: 9653\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t19\n",
      "Len: 9166\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t20\n",
      "Len: 9265\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t21\n",
      "Len: 9062\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t22\n",
      "Len: 9411\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t23\n",
      "Len: 9313\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t24\n",
      "Len: 9337\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t25\n",
      "Len: 9571\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t26\n",
      "Len: 9530\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t27\n",
      "Len: 9069\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t28\n",
      "Len: 9739\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t29\n",
      "Len: 9373\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t30\n",
      "Len: 9189\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t31\n",
      "Len: 9238\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t32\n",
      "Len: 9187\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t33\n",
      "Len: 9327\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t34\n",
      "Len: 9283\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t35\n",
      "Len: 9577\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t36\n",
      "Len: 9259\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t37\n",
      "Len: 9724\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t38\n",
      "Len: 9494\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t39\n",
      "Len: 9727\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t40\n",
      "Len: 9486\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t41\n",
      "Len: 9161\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t42\n",
      "Len: 9385\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t43\n",
      "Len: 9549\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t44\n",
      "Len: 9426\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t45\n",
      "Len: 9591\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t46\n",
      "Len: 9622\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t47\n",
      "Len: 9594\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t48\n",
      "Len: 9362\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t49\n",
      "Len: 9443\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t50\n",
      "Len: 9390\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t51\n",
      "Len: 9373\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t52\n",
      "Len: 9418\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t53\n",
      "Len: 9890\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t54\n",
      "Len: 9719\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t55\n",
      "Len: 9902\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t56\n",
      "Len: 9200\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t57\n",
      "Len: 9676\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t58\n",
      "Len: 9417\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t59\n",
      "Len: 9486\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t60\n",
      "Len: 9662\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t61\n",
      "Len: 9667\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t62\n",
      "Len: 9730\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t63\n",
      "Len: 9258\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t64\n",
      "Len: 9314\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t65\n",
      "Len: 9159\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t66\n",
      "Len: 9382\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t67\n",
      "Len: 9457\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t68\n",
      "Len: 9353\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t69\n",
      "Len: 9517\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t70\n",
      "Len: 9237\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t71\n",
      "Len: 9417\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t72\n",
      "Len: 9659\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t73\n",
      "Len: 9572\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t74\n",
      "Len: 9192\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t75\n",
      "Len: 9753\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t76\n",
      "Len: 9383\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t77\n",
      "Len: 9621\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t78\n",
      "Len: 9400\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t79\n",
      "Len: 9477\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t80\n",
      "Len: 9571\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t81\n",
      "Len: 9243\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t82\n",
      "Len: 9133\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t83\n",
      "Len: 9306\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t84\n",
      "Len: 9609\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t85\n",
      "Len: 9359\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t86\n",
      "Len: 9396\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t87\n",
      "Len: 9726\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t88\n",
      "Len: 9464\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t89\n",
      "Len: 9417\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t90\n",
      "Len: 9380\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t91\n",
      "Len: 9185\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t92\n",
      "Len: 9017\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t93\n",
      "Len: 9564\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t94\n",
      "Len: 9790\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t95\n",
      "Len: 9173\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t96\n",
      "Len: 9151\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t97\n",
      "Len: 9342\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t98\n",
      "Len: 9721\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n",
      "Iteration:\t99\n",
      "Len: 9533\n",
      "Train\n",
      "Test\n",
      "LogReg\n",
      "SVM\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "SVM_log    = \"\"\n",
    "LOGREG_log = \"\"\n",
    "NN_log     = \"\"\n",
    "for e in range(100):\n",
    "    print(\"Iteration:\\t\"+str(e))\n",
    "    df_train, df_test  = getTrainTest(seed = e)\n",
    "    dtype = 2\n",
    "    all_words = get_words_matrix(df_train, show_progress = False)\n",
    "    print('Train')\n",
    "    X_train = get_X_matrix(df_train, all_words, show_progress = False)\n",
    "    print('Test')\n",
    "    X_test  = get_X_matrix(df_test, all_words, show_progress = False)\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_test['label']\n",
    "    \n",
    "    print('LogReg')\n",
    "    LOGREG_log += LogisticRegressionScore(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"SVM\")\n",
    "    SVM_log += SVMScore(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"Neural network\")\n",
    "    NN_log += NNScore(X_train, y_train, X_test, y_test, verbose=0, epochs=13)\n",
    "\n",
    "\n",
    "    open('logs/LogReglogsW2V_vkmy.txt','w+').write(LOGREG_log)\n",
    "    open('logs/SVMlogsW2V_vkmy.txt','w+').write(SVM_log)\n",
    "    open('logs/NNlogsW2V_vkmy.txt','w+').write(NN_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
