{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.tokenize as word_tokenize\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\") # Choose a language\n",
    "\n",
    "FIN      = 'data/data.csv'\n",
    "FOUT     = 'data/data_clear.csv'\n",
    "FOUTSPAM = 'data/data_spam.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(FIN) #FULL dirty dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Очистим датасет от дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_me(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text, language='english')\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    stop_words = stopwords.words('russian')\n",
    "    stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на','-','br'])\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "    return tokens\n",
    "def decode(a, encoding=\"utf8\"):\n",
    "    if isinstance(a, bytes):\n",
    "        return a.decode(encoding)\n",
    "    else:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_shingles_lst(s):\n",
    "    s = decode(s).replace('http://','').replace('https://','').replace('<br>','')\n",
    "    s = s.replace('  ',' ').replace('  ',' ').replace('  ',' ').split(' ')\n",
    "    return list(filter(lambda x:not u'#' in x, s))\n",
    "\n",
    "N = 4          # shingle len\n",
    "all_shingles = Counter()\n",
    "for S in df['data'].get_values()[2:]:\n",
    "    words = get_shingles_lst(S)\n",
    "    #   filter(lambda x:len(x)>2,decode(S).lower().replace('href=http','').replace('<br>','').split(' '))\n",
    "    words = list(filter(lambda x:len(x)>2 and x[0]!=u'#', words))\n",
    "    shingles_temp = []\n",
    "    for c in range(max(0,len(words)-N)):\n",
    "        temp = \" \".join([words[c+c1] for c1 in range(N)])#words[c]+' '+words[c+1]+' '+words[c+2]\n",
    "\n",
    "        shingles_temp.append(temp)\n",
    "    \n",
    "    for hsh in set(shingles_temp):\n",
    "        all_shingles[hsh] += 1\n",
    "\n",
    "all_shingles['- - -'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено дубликатов:\n",
      "По шинглам\t\t\t\t230\n",
      "По простой ключевой фразе vk.com/app\t447\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 1\n",
    "bad_posts = []\n",
    "max_print = 0\n",
    "c_bad = 0\n",
    "for S in df['data'].get_values():\n",
    "    if max_print>10:\n",
    "        break\n",
    "    temp_len = 0\n",
    "    temp = []\n",
    "    words = get_shingles_lst(S)\n",
    "    for c in range(max(0,len(words)-N)):\n",
    "        temp = \" \".join([words[c+c1] for c1 in range(N)])\n",
    "        if all_shingles[temp] > 1:# or u'vk.com/ap1klsjda2p' in S:\n",
    "            temp_len+=1\n",
    "    if temp_len>1:\n",
    "        bad_posts.append(1)\n",
    "    else:\n",
    "        if u'vk.com/app' in S:\n",
    "            c_bad+=1\n",
    "        bad_posts.append(0)\n",
    "print(u'Найдено дубликатов:')\n",
    "print(u\"По шинглам\\t\\t\\t\\t\"+str(sum(bad_posts)))\n",
    "print(u\"По простой ключевой фразе vk.com/app\\t\"+str(c_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чистые дупликаты:\t151\n"
     ]
    }
   ],
   "source": [
    "a = set(df['data'])\n",
    "b = list(df['data'])\n",
    "bad_dublicate_rows = []\n",
    "for txt in a:\n",
    "    if b.count(txt) != 1:\n",
    "        lbl = df[df['data']==txt]['label']\n",
    "        bad_single = False\n",
    "        for i in list(lbl)[1:]:\n",
    "            if i != list(lbl)[0]:\n",
    "                bad_single = True\n",
    "        if bad_single:\n",
    "            bad_dublicate_rows.extend(lbl.index)\n",
    "        else:\n",
    "            bad_dublicate_rows.extend(lbl[1:].index)\n",
    "print(\"Чистые дупликаты:\\t\"+str(len(df.iloc[bad_dublicate_rows])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def in_post(post, lst = u'vk.com/app\\thttp://vk.com/indykot\\t очков на '):\n",
    "    bad = False\n",
    "    for i in lst.split('\\t'):\n",
    "        if i in post:\n",
    "            bad = True\n",
    "    return bad\n",
    "\n",
    "bad_posts_index = bad_dublicate_rows\n",
    "for c,S in enumerate(df['data'].get_values()):\n",
    "    if in_post(S):\n",
    "#     bad_posts[c]==1 or u'vk.com/app' in S or u'http://vk.com/indykot' in S or u' очков на ' in S: #or big_post(S): delete long posts\n",
    "        bad_posts_index.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_mask = df.index.isin(bad_posts_index)\n",
    "df_spam = df[row_mask]  # Спам\n",
    "\n",
    "row_mask = df.index.isin(list(set(range(len(df)))-set(bad_posts_index)))\n",
    "df_all = df[row_mask]   # Чистые посты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранить полученные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_spam.to_csv(FOUTSPAM,     index = False)\n",
    "df_all.to_csv( FOUT, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшая статистика по удаленным дубликатам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В спаме:   628\n",
      "\t-1\t11\n",
      "\t0\t491\n",
      "\t1\t126\n",
      "В выборке: 4247\n",
      "\t-1\t339\n",
      "\t0\t2620\n",
      "\t1\t1288\n",
      "===\n",
      "Длинных постов(не в спаме): \n",
      "\t-1\t12\t3.5398230088495577%\n",
      "\t0\t188\t7.175572519083969%\n",
      "\t1\t17\t1.3198757763975155%\n",
      "  Sum 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "def big_post(s, max_len = 128):\n",
    "    return s.replace('  ',' ').replace('  ',' ').replace('  ',' ').count(u' ')>128\n",
    "\n",
    "print(u'В спаме:   '+str(len(df_spam)))\n",
    "for i in [-1,0,1]:\n",
    "    print(u'\\t'+str(i)+u'\\t'+str(list(df_spam['label'].get_values()).count(i)))\n",
    "\n",
    "print(u'В выборке: '+str(len(df_all)))\n",
    "for i in [-1,0,1]:\n",
    "    print(u'\\t'+str(i)+u'\\t'+str(list(df_all['label'].get_values()).count(i)))\n",
    "    \n",
    "print(u'===\\nДлинных постов(не в спаме): ')\n",
    "len_all = 0\n",
    "for ii in [-1,0,1]:\n",
    "    len_of_words=0\n",
    "    for i in df_all[df['label']==ii]['data']:\n",
    "        if big_post(i):\n",
    "            len_of_words+=1\n",
    "    len_all += len_of_words      \n",
    "    print(u'\\t'+str(ii)+u'\\t'+str(len_of_words)+'\\t'+str(len_of_words*100.0/len(df_all[df['label']==ii]['data']))+'%')\n",
    "print(u'  Sum '+str(len_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
